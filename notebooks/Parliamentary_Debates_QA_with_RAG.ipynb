{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORScH4RTXbQkd/m9BRSDt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pk2971/computational-gender-analysis/blob/main/notebooks/Parliamentary_Debates_QA_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs9wVA1Za30C"
      },
      "outputs": [],
      "source": [
        "%pip install -U langchain-community\n",
        "%pip install -U sentence-transformers\n",
        "%pip install chromadb\n",
        "%pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import re\n",
        "from typing import Union, List\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain.vectorstores import Chroma\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "QivMgjiba_Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_xml(xml_content: str) -> str:\n",
        "    soup = BeautifulSoup(xml_content, \"lxml-xml\")\n",
        "    return soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "def load_and_split_xml_from_zip(\n",
        "    zip_path: str,\n",
        "    years: Union[int, List[int]],\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200\n",
        "):\n",
        "    if isinstance(years, int):\n",
        "        years = [years]\n",
        "    year_pattern = '|'.join(str(y) for y in years)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_file:\n",
        "        matched_files = [\n",
        "            f for f in zip_file.namelist()\n",
        "            if re.search(rf'debates({year_pattern})-\\d{{2}}-\\d{{2}}a\\.xml', f)\n",
        "        ]\n",
        "\n",
        "        all_texts = []\n",
        "        for filename in matched_files:\n",
        "            with zip_file.open(filename) as file:\n",
        "                xml_content = file.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "                text = extract_text_from_xml(xml_content)\n",
        "                all_texts.append(text)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.create_documents(all_texts)\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "aUBHoKKAa_aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change year of documents to load here.\n",
        "zip_file_path = \"/content/drive/MyDrive/debates.zip\"\n",
        "years = 1928\n",
        "docs = load_and_split_xml_from_zip(zip_file_path, years)\n"
      ],
      "metadata": {
        "id": "o6iMxoiva_cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n"
      ],
      "metadata": {
        "id": "JU1nB_b7bFyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "access_token = \"HF token\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=access_token,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "LnKlXX2VbHj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the answer with context from the Debates for cross referencing\n",
        "def gradio_rag_bot_context(query):\n",
        "    # Retrieve and rerank\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=15)\n",
        "    texts = [doc.page_content for doc in retrieved_docs]\n",
        "    pairs = [[query, t] for t in texts]\n",
        "    scores = reranker.predict(pairs)\n",
        "    ranked = sorted(zip(texts, scores), key=lambda x: x[1], reverse=True)\n",
        "    top_contexts = [t for t, s in ranked[:5]]\n",
        "    context = \"\\n\".join(top_contexts)\n",
        "    # Improved prompt\n",
        "    prompt = (\n",
        "    \"You are a historian assistant. Based on the context below, answer the question.\\n\"\n",
        "    \"Context:\\n\"\n",
        "    f\"{context}\\n\\n\"\n",
        "    f\"Question: {query}\\n\"\n",
        "    \"Answer (as bullet points):\"\n",
        ")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(**inputs, max_new_tokens=256)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Show context for debugging\n",
        "    return f\"ANSWER:\\n{answer}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_rag_bot_context,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Ask a question about the debates\"),\n",
        "    outputs=gr.Textbox(lines=20, label=\"Debug Output (Context + Answer)\"),\n",
        "    title=\"Debates RAG Chatbot (Debug Mode)\",\n",
        "    description=\"See the context the model is using. If the answer is off, the context is likely not relevant.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "o0KXSQT9bRZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_rag_bot(query):\n",
        "    # Retrieve and rerank\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=15)\n",
        "    texts = [doc.page_content for doc in retrieved_docs]\n",
        "    pairs = [[query, t] for t in texts]\n",
        "    scores = reranker.predict(pairs)\n",
        "    ranked = sorted(zip(texts, scores), key=lambda x: x[1], reverse=True)\n",
        "    top_contexts = [t for t, s in ranked[:5]]\n",
        "    context = \"\\n\".join(top_contexts)\n",
        "\n",
        "    # Build improved prompt\n",
        "    prompt = (\n",
        "        \"You are a historian assistant. Based on the context below, answer the question.\\n\"\n",
        "        \"Context:\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer (as bullet points):\"\n",
        "    )\n",
        "\n",
        "    # Generate answer\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(**inputs, max_new_tokens=512)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return only the clean answer\n",
        "\n",
        "    if \"Answer (as bullet points):\" in answer:\n",
        "      answer = answer.split(\"Answer (as bullet points):\")[-1].strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "r2c6JIKXbU-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=gradio_rag_bot,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Ask a question about the debates\"),\n",
        "    outputs=gr.Textbox(lines=20, label=\"Answer\"),\n",
        "    title=\"Debates RAG Chatbot\",\n",
        "    description=\"Ask about historical UK Parliament debates (1919â€“2024).\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "9ZqctbGobX_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch(share=True)  # 'share=True' gives you a public link in Colab\n"
      ],
      "metadata": {
        "id": "HVC9og6xbZwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}