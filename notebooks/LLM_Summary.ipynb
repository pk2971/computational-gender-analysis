{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzkObPzVVbfq+S62rQVt/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pk2971/computational-gender-analysis/blob/main/LLM_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGZOlDLZznFJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from bs4 import BeautifulSoup\n",
        "import zipfile\n",
        "import re\n",
        "from typing import Union, List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "id": "ceztvJ2azvdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Load + Split XML from ZIP\n",
        "# ---------------------------\n",
        "def extract_text_from_xml(xml_content: str) -> str:\n",
        "    soup = BeautifulSoup(xml_content, \"lxml-xml\")\n",
        "    return soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "def load_and_split_xml_from_zip(\n",
        "    zip_path: str,\n",
        "    years: Union[int, List[int]],\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200\n",
        "):\n",
        "    if isinstance(years, int):\n",
        "        years = [years]\n",
        "    year_pattern = '|'.join(str(y) for y in years)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_file:\n",
        "        matched_files = [\n",
        "            f for f in zip_file.namelist()\n",
        "            if re.search(rf'debates({year_pattern})-\\d{{2}}-\\d{{2}}a\\.xml', f)\n",
        "        ]\n",
        "\n",
        "        all_texts = []\n",
        "        for filename in matched_files:\n",
        "            with zip_file.open(filename) as file:\n",
        "                xml_content = file.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "                text = extract_text_from_xml(xml_content)\n",
        "                all_texts.append(text)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.create_documents(all_texts)\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "n5qeVDHTzo4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"/content/drive/MyDrive/debates.zip\"  # Replace with your path\n",
        "docs = load_and_split_xml_from_zip(zip_file_path, 2022)"
      ],
      "metadata": {
        "id": "r8tGWDsTzsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Vector store setup\n",
        "# ---------------------------\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "# ---------------------------\n",
        "# Load LLaMA-3 model\n",
        "# ---------------------------\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "access_token = \" \"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=access_token,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, temperature=0.2, top_k=50)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# ---------------------------\n",
        "# Prompt Template + QA Chain\n",
        "# ---------------------------\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant trained to answer questions using excerpts from British parliamentary debate texts from the years provided.\n",
        "Your goal is to extract or infer answers based strictly on the retrieved documents.\n",
        "\n",
        "— If the answer is explicitly stated, quote the relevant parts.\n",
        "— If it's implied (e.g. tone, sentiment, theme), explain your reasoning clearly.\n",
        "— Do not repeat the same quote or sentence.\n",
        "— If the information is not available, say: 'There is no information regarding this in the given text.'\n",
        "\n",
        "Be accurate, concise, and clear in your response.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "MqCdODRjztYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_sentences(text):\n",
        "    seen = set()\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    unique_sentences = []\n",
        "    for s in sentences:\n",
        "        cleaned = s.strip()\n",
        "        if cleaned and cleaned.lower() not in seen:\n",
        "            seen.add(cleaned.lower())  # lowercase for fuzzy dedup\n",
        "            unique_sentences.append(cleaned)\n",
        "    return '\\n'.join(unique_sentences)\n",
        "\n",
        "def generate_response(query):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
        "    query_with_prompt = prompt_template + \"\\n\\nQuestion: \" + query\n",
        "    response = qa_chain.run(query_with_prompt)\n",
        "    response = response.split('Answer:')[1].strip() if 'Answer:' in response else response.strip()\n",
        "    return remove_duplicate_sentences(response)\n"
      ],
      "metadata": {
        "id": "hPOcjLp1z0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pretty_response(response, width=100):\n",
        "    import textwrap\n",
        "    wrapper = textwrap.TextWrapper(width=width)\n",
        "    print(\"\\n\" + \"\\n\".join(wrapper.wrap(response)) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "sWqHQyxVz183"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# CLI Loop\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        query = input(\"Ask a question (or type 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        response = generate_response(query)\n",
        "        print(\"Chatbot:\", print_pretty_response(response))"
      ],
      "metadata": {
        "id": "NkEv2e2Wz2dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}