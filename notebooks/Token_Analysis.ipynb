{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tGt085NuCKvwSDS-FyEpdlfdJmTvsHx-",
      "authorship_tag": "ABX9TyPN0TtLclasq4qqQBoKahMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pk2971/computational-gender-analysis/blob/main/notebooks/Token_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F44KAGgYw3-2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id22G0zp3NgH",
        "outputId": "fc81eec2-e02e-4b21-f7c6-d106846c2e8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/debates.zip'\n",
        "# Clean and parse XML text\n",
        "def extract_text_from_speech(xml_bytes):\n",
        "    try:\n",
        "        root = ET.fromstring(xml_bytes)\n",
        "        return ' '.join([p.text or '' for p in root.findall('.//speech//p')])\n",
        "    except ET.ParseError:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "zS78ZWbL0sME"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_year_from_filename(filename):\n",
        "    match = re.search(r'debates(\\d{4})', filename)\n",
        "    return int(match.group(1)) if match else None\n"
      ],
      "metadata": {
        "id": "6kecp5RM3XAr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_documents_by_year(zip_path, start_year, end_year):\n",
        "    year_docs = defaultdict(str)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        for file_name in zip_ref.namelist():\n",
        "            if not file_name.endswith('.xml'):\n",
        "                continue\n",
        "            year = get_year_from_filename(file_name)\n",
        "            if year and start_year <= year <= end_year:\n",
        "                with zip_ref.open(file_name) as f:\n",
        "                    xml_bytes = f.read()\n",
        "                    year_docs[year] += extract_text_from_speech(xml_bytes) + \" \"\n",
        "    return year_docs"
      ],
      "metadata": {
        "id": "12GllqC33a55"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_tfidf_words(zip_path, year_input, top_n=20, extra_stopwords=[]):\n",
        "    # Parse year range\n",
        "    if isinstance(year_input, int):\n",
        "        start_year, end_year = year_input, year_input\n",
        "    else:\n",
        "        start_year, end_year = year_input\n",
        "\n",
        "    # Collect documents by year\n",
        "    year_docs = collect_documents_by_year(zip_path, start_year, end_year)\n",
        "    years = sorted(year_docs.keys())\n",
        "    documents = [year_docs[year] for year in years]\n",
        "    extended_stopwords = [\n",
        "    # Standard English stopwords (already in NLTK/Scikit-learn)\n",
        "    # Custom additions:\n",
        "    \"shall\", \"may\", \"also\", \"must\", \"however\", \"therefore\", \"thus\", \"hereby\", \"whereas\", \"therein\",\n",
        "    \"thereof\", \"herein\", \"therewith\", \"thereupon\", \"hereafter\", \"herewith\", \"hereupon\", \"hereinbefore\",\n",
        "    \"hereinafter\", \"hence\", \"furthermore\", \"further\", \"meanwhile\", \"nevertheless\", \"nonetheless\",\n",
        "    \"notwithstanding\", \"wherein\", \"whereby\", \"whereupon\", \"whereof\", \"whilst\", \"amongst\", \"upon\",\n",
        "    \"whereas\", \"aforementioned\", \"said\", \"such\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\n",
        "    \"eight\", \"nine\", \"ten\", \"first\", \"second\", \"third\", \"forth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\",\n",
        "    \"ninth\", \"tenth\", \"mr\", \"mrs\", \"ms\", \"hon\", \"honourable\", \"gentleman\", \"gentlemen\", \"lady\", \"lord\",\n",
        "    \"member\", \"members\", \"committee\", \"house\", \"assembly\", \"parliament\", \"bill\", \"act\", \"motion\",\n",
        "    \"debate\", \"question\", \"answer\", \"clause\", \"subsection\", \"section\", \"amendment\", \"order\", \"session\",\n",
        "    \"speaker\", \"government\", \"minister\", \"secretary\", \"right\", \"left\", \"mr.\", \"mrs.\", \"ms.\", \"hon.\",\n",
        "    \"honourable.\", \"gentleman.\", \"gentlemen.\", \"lady.\", \"lord.\", \"member.\", \"members.\", \"committee.\",\n",
        "    \"house.\", \"assembly.\", \"parliament.\", \"bill.\", \"act.\", \"motion.\", \"debate.\", \"question.\", \"answer.\",\n",
        "    \"clause.\", \"subsection.\", \"section.\", \"amendment.\", \"order.\", \"session.\", \"speaker.\", \"government\",\n",
        "    \"minister.\", \"secretary\",\n",
        "    # Modal verbs, formalities, and filler words\n",
        "    \"would\", \"could\", \"should\", \"might\", \"can\", \"will\", \"do\", \"does\", \"did\", \"done\", \"being\", \"having\",\n",
        "    \"make\", \"made\", \"take\", \"taken\", \"give\", \"given\", \"get\", \"got\", \"go\", \"going\", \"say\", \"said\", \"come\",\n",
        "    \"came\", \"see\", \"seen\", \"think\", \"thought\", \"know\", \"known\", \"let\", \"lets\", \"much\", \"many\", \"few\",\n",
        "    \"several\", \"any\", \"every\", \"each\", \"other\", \"others\", \"another\", \"more\", \"most\", \"some\", \"anyone\",\n",
        "    \"anything\", \"everyone\", \"everything\", \"none\", \"nothing\", \"always\", \"never\", \"sometimes\", \"often\",\n",
        "    \"already\", \"yet\", \"still\", \"just\", \"even\", \"ever\", \"back\", \"forward\", \"again\", \"new\", \"old\",\n",
        "    \"whether\",\"put\",\"cannot\",\"like\",\"though\",\"asked\",\n",
        "\n",
        "    # Numbers as words\n",
        "    \"zero\", \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
        "\n",
        "\n",
        "    # Prepare stopwords\n",
        "    stop_words = list(set(stopwords.words('english')).union(set(extra_stopwords)).union(set(extended_stopwords)))\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get top words for each year\n",
        "    for idx, year in enumerate(years):\n",
        "        tfidf_scores = tfidf_matrix[idx].toarray().flatten()\n",
        "        top_indices = tfidf_scores.argsort()[::-1][:top_n]\n",
        "        top_words = [(feature_names[i], tfidf_scores[i]) for i in top_indices if tfidf_scores[i] > 0]\n",
        "        print(f\"\\nTop words for {year}:\")\n",
        "        for word, score in top_words:\n",
        "            print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "JTN7KjmT0sgl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_top_tfidf_words('/content/drive/MyDrive/debates.zip', 1919, top_n=50, extra_stopwords=[\"000\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7hDttkc2dxQ",
        "outputId": "c1d7e671-c5f3-4515-ef68-79d8e7dfb131"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top words for 1919:\n",
            "war: 0.2260\n",
            "country: 0.2133\n",
            "time: 0.1820\n",
            "friend: 0.1750\n",
            "men: 0.1748\n",
            "great: 0.1655\n",
            "us: 0.1231\n",
            "present: 0.1222\n",
            "matter: 0.1198\n",
            "people: 0.1187\n",
            "state: 0.1150\n",
            "board: 0.1089\n",
            "case: 0.1040\n",
            "trade: 0.1028\n",
            "regard: 0.0990\n",
            "work: 0.0977\n",
            "part: 0.0959\n",
            "point: 0.0954\n",
            "way: 0.0949\n",
            "day: 0.0930\n",
            "view: 0.0896\n",
            "whole: 0.0866\n",
            "years: 0.0846\n",
            "hope: 0.0842\n",
            "quite: 0.0827\n",
            "far: 0.0799\n",
            "local: 0.0798\n",
            "possible: 0.0791\n",
            "man: 0.0781\n",
            "service: 0.0781\n",
            "want: 0.0780\n",
            "position: 0.0778\n",
            "land: 0.0778\n",
            "general: 0.0768\n",
            "last: 0.0761\n",
            "public: 0.0745\n",
            "number: 0.0745\n",
            "year: 0.0739\n",
            "fact: 0.0724\n",
            "british: 0.0712\n",
            "certain: 0.0710\n",
            "per: 0.0688\n",
            "deal: 0.0677\n",
            "labour: 0.0661\n",
            "gallant: 0.0648\n",
            "army: 0.0634\n",
            "pay: 0.0633\n",
            "aware: 0.0628\n",
            "without: 0.0626\n",
            "money: 0.0622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1RT6J6k2jkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}